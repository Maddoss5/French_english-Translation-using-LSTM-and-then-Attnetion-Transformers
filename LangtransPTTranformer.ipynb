{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fd3d8b-bef9-4182-9c47-04ed5b94f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import Datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d41ae3-f85b-4266-976e-115410421201",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55619e2a-5124-4c0d-b365-a0268bffca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812e5bfb-6be0-4cf7-bd9a-838083f45a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('engfrench.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9de6cd-6070-4bb0-8a97-c5cfb6d3cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175621\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16df2aed-79f6-4a10-af85-e10d4a10a9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    The guards found a hacksaw blade in the prison...\n",
       "French words/sentences     Les gardiens trouvèrent une lame de scie à mét...\n",
       "Name: 170000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[170000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025efad2-8e80-47f1-a486-64a94073dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Be very careful.</td>\n",
       "      <td>Sois très prudente !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Bees make honey.</td>\n",
       "      <td>Les abeilles font du miel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Behave yourself.</td>\n",
       "      <td>Comporte-toi bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Bite the bullet.</td>\n",
       "      <td>Serre les dents.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Bite the bullet.</td>\n",
       "      <td>Serrez les dents.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English words/sentences      French words/sentences\n",
       "10000        Be very careful.        Sois très prudente !\n",
       "10001        Bees make honey.  Les abeilles font du miel.\n",
       "10002        Behave yourself.          Comporte-toi bien.\n",
       "10003        Bite the bullet.            Serre les dents.\n",
       "10004        Bite the bullet.           Serrez les dents."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10000:10005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe24555-676c-440b-b2bd-4876a36cc782",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe1ff5-c1f9-4b8b-aec0-a4e907ffceb4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d31611e-192e-4b2a-bafb-06ca5a82ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID=0\n",
    "SOS_TOKEN_ID = 1\n",
    "EOS_TOKEN_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad1c64-dd2d-4601-b8f5-df4f66600163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ed24b-8349-4eb7-b1d3-bbc3e5a9aa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf3179-1b66-40fb-adac-a22ca0fc5971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f06527-e676-4c98-962a-2c4f64ec51b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8165ff4-0c6a-443e-b3bc-7b9defd201fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txten=data.iloc[0,0]\n",
    "txtfr=data.iloc[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841c7d65-e445-45da-9b8c-605193c87e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b3ef1e-ee68-42a6-add8-02108412f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salut!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b9abb0-f1ad-400b-934e-4c6c46c165ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1505539583.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  en_check=torch.load('engEmbeddings.pt')\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1505539583.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fr_check=torch.load('frEmbeddings.pt')\n"
     ]
    }
   ],
   "source": [
    "en_check=torch.load('engEmbeddings.pt')\n",
    "fr_check=torch.load('frEmbeddings.pt')\n",
    "en_weights=en_check['weights']\n",
    "en_vocab=en_check['vocab_dict']\n",
    "fr_weights=fr_check['weights']\n",
    "fr_vocab=fr_check['vocab_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfb4c38-f5c7-4858-8734-f1ee732f9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, en_vocab, fr_vocab):\n",
    "        self.df = dataframe\n",
    "        self.en_vocab = en_vocab\n",
    "        self.fr_vocab = fr_vocab\n",
    "        \n",
    "        # Define special token IDs\n",
    "        # IMPORTANT: Ensure these keys exist in your vocab dictionaries!\n",
    "        self.unk_idx_en = en_vocab.get('<unk>', 0) # Default to 0 if not found\n",
    "        self.eos_idx_en = en_vocab.get('<eos>', 2)\n",
    "        \n",
    "        self.unk_idx_fr = fr_vocab.get('<unk>', 0)\n",
    "        self.sos_idx_fr = fr_vocab.get('<sos>', 1)\n",
    "        self.eos_idx_fr = fr_vocab.get('<eos>', 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Get Raw Text\n",
    "        src_text = str(self.df.iloc[index, 0])\n",
    "        trg_text = str(self.df.iloc[index, 1])\n",
    "\n",
    "        # 2. Tokenize (using your Spacy functions)\n",
    "        # We use .lower() because FastText matches better with lowercase\n",
    "        src_tokens = tokenize_en(src_text.lower())\n",
    "        trg_tokens = tokenize_fr(trg_text.lower())\n",
    "\n",
    "        # 3. Numericalize Source (English)\n",
    "        # Use .get() to map unknown words to <unk> index\n",
    "        src_indices = [self.en_vocab.get(token, self.unk_idx_en) for token in src_tokens]\n",
    "        \n",
    "        # 4. Numericalize Target (French)\n",
    "        trg_indices = [self.fr_vocab.get(token, self.unk_idx_fr) for token in trg_tokens]\n",
    "\n",
    "        # 5. Add Special Tokens\n",
    "        # Source usually needs <eos> at the end so the LSTM knows the sentence stopped\n",
    "        src_indices.append(self.eos_idx_en)\n",
    "        \n",
    "        # Target needs <sos> to start generation and <eos> to end it\n",
    "        trg_indices = [self.sos_idx_fr] + trg_indices + [self.eos_idx_fr]\n",
    "\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d80f4-c634-4be8-90ec-24c4e631caa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ea23429-fdac-48fe-829b-59ed9a153ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanewdataset=TranslationDataset(data,en_vocab,fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bccd8c-4881-409a-8e1f-e4e50fff387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # 'batch' is a list of tuples: [(input_tensor_1, target_tensor_1), (input_tensor_2, target_tensor_2), ...]\n",
    "\n",
    "    # 1. Separate inputs and targets\n",
    "    inputs = [torch.tensor(item[0]) for item in batch]\n",
    "    targets = [torch.tensor(item[1]) for item in batch]\n",
    "\n",
    "    # 2. Pad the inputs and targets to the length of the longest sequence in the batch\n",
    "    # batch_first=True makes the output shape (BatchSize, SequenceLength)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    \n",
    "    return inputs_padded, targets_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b51b6de-3ca3-481d-9119-6dc6772d3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "haha=DataLoader(datanewdataset,batch_size=32,shuffle=True,collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce07bd-19b3-43ad-a820-3e009ea3ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d26a01-a545-495f-b951-878ee45e667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b364586-8172-4ecf-96db-b1e781b20ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62a31a-6579-4f09-a820-810e546e45ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe745ea-9a11-4802-9f5f-cbf53428ed04",
   "metadata": {},
   "source": [
    "SAMPLE TO REMEMBER LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf9879c-de35-4cb7-b158-b51608fd6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa=torch.randn(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a435a35-e05b-4e98-8c7a-d28669943f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ha=nn.LSTM(100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e076d8cb-7094-4dea-8a85-af2cf4064cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ya=ha(xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f497155-a726-4de9-a4f9-dc4b85034d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3400ae-aaa7-4f69-965a-a7b0e64e4bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da92149f-7fcc-44a4-b1cb-ca91e63dee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063b89a-269e-4954-955d-11b2737534ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eee28f-7402-4731-b185-2ab135621136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a256-a5ee-432a-9aa0-9f8b3afcf0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01357329-82cc-4c9a-9a8b-b16da3c77f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=300\n",
    "#since using pre trained Fast TExt aligned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cc29a79-b82a-461d-bd5a-56f3f0f5809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_ENC=len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acde860c-f52a-4ac4-8001-d654390c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_FR=len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95e5e1-d7c6-4a3e-a2dc-2e785ae9637a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd32de2a-d261-47bb-87cd-69cf7ffffa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long constant matrix representing position\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a learnable parameter, but part of state_dict)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        # Add position encoding to embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9442f3c2-c760-4990-88da-61b1c5415c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, device, embed_dim=300, nhead=4, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Embeddings (Using your pretrained weights)\n",
    "        self.src_emb = nn.Embedding.from_pretrained(en_weights, freeze=True)\n",
    "        self.tgt_emb = nn.Embedding.from_pretrained(fr_weights, freeze=True)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        \n",
    "        # 3. The Transformer\n",
    "        # batch_first=True is crucial to match your [batch, len] data format\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        # 4. Output Layer\n",
    "        self.fc_out = nn.Linear(embed_dim, len(fr_vocab)) # Project to French Vocab size\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch, src_len]\n",
    "        # tgt: [batch, tgt_len] (This is the \"Teacher Forcing\" input)\n",
    "        \n",
    "        # Create Masks (Crucial for Transformers!)\n",
    "        # src_mask = mask for padding tokens in source\n",
    "        # tgt_mask = \"Causal Mask\" (prevents decoder from seeing future words) + padding mask\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(self.device)\n",
    "        \n",
    "        # Generate Padding Masks (assuming 0 is your pad_token_id)\n",
    "        # These boolean masks tell the model to ignore padding tokens\n",
    "        src_padding_mask = (src == 0).to(self.device) \n",
    "        tgt_padding_mask = (tgt == 0).to(self.device)\n",
    "\n",
    "        # Apply Embeddings + Positional Encoding\n",
    "        src_emb = self.positional_encoding(self.src_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_emb(tgt))\n",
    "        \n",
    "        # Pass through Transformer\n",
    "        outs = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=tgt_mask, # Mask future positions\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask # Mask source padding in attention\n",
    "        )\n",
    "        \n",
    "        # Final Projection\n",
    "        return self.fc_out(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b01eec7-5d21-4ade-97c3-5af4f22a315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # RTX 4070 supports Mixed Precision (faster training)\n",
    "    scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device) # trg shape: [batch, trg_len] e.g., <sos> ... <eos>\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare Inputs and Targets for Transformer\n",
    "        # Decoder Input: Remove the LAST token (<eos>)\n",
    "        tgt_input = trg[:, :-1] \n",
    "        \n",
    "        # Expected Output: Remove the FIRST token (<sos>)\n",
    "        tgt_output = trg[:, 1:] \n",
    "        \n",
    "        with torch.cuda.amp.autocast(): # Mixed precision context\n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Output shape: [batch, seq_len-1, vocab_size]\n",
    "            # Reshape for Loss: [batch * (seq_len-1), vocab_size]\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backprop with Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip Gradients (Good stability for Transformers too)\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Mean Epoch Loss: {epoch_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85ae9e74-19c8-48d8-9779-4b37465eee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_translation_transformer(model, sentence, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Preprocess (Tokenize + Numericalize)\n",
    "    tokens = tokenize_en(sentence.lower())\n",
    "    unk_idx = en_vocab.get('<unk>', 3)\n",
    "    eos_idx = en_vocab.get('<eos>', 2)\n",
    "    ids = [en_vocab.get(token, unk_idx) for token in tokens]\n",
    "    ids.append(eos_idx) # Add <eos> to source\n",
    "    \n",
    "    src_tensor = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 2. The Loop\n",
    "    # Start with just the <sos> token\n",
    "    sos_idx = fr_vocab['<sos>']\n",
    "    eos_idx = fr_vocab['<eos>']\n",
    "    \n",
    "    tgt_indices = [sos_idx]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get masks inside the model forward (or create dummy masks here)\n",
    "            # We call the model. The model will create the causal mask internally\n",
    "            output = model(src_tensor, tgt_tensor)\n",
    "        \n",
    "        # We only care about the prediction for the LAST token\n",
    "        # output shape: [1, len, vocab]\n",
    "        last_token_logits = output[:, -1, :]\n",
    "        pred_token = last_token_logits.argmax(1).item()\n",
    "        \n",
    "        tgt_indices.append(pred_token)\n",
    "        \n",
    "        if pred_token == eos_idx:\n",
    "            break\n",
    "            \n",
    "    # 3. Decode to Words\n",
    "    idx_to_word = {v: k for k, v in fr_vocab.items()}\n",
    "    predicted_tokens = [idx_to_word.get(idx, '<unk>') for idx in tgt_indices[1:]] # Skip <sos>\n",
    "    \n",
    "    # Remove <eos> from display if present\n",
    "    if predicted_tokens and predicted_tokens[-1] == '<eos>':\n",
    "        predicted_tokens = predicted_tokens[:-1]\n",
    "        \n",
    "    return \" \".join(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be6cc9f8-b1cf-48d9-b33d-c774c8b3ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "856766e1-5deb-4792-b5d9-0b320cbc6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on: {torch.cuda.get_device_name(0)}\") # Should say RTX 4070\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    device=device,\n",
    "    embed_dim=embed_dim, # e.g. 300\n",
    "    nhead=4,             # 300 / 4 = 75\n",
    "    num_layers=3,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Assuming 0 is pad_index\n",
    "\n",
    "# Run Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7910d65-c214-4632-80f1-789c125c46a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\3600398038.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1408414250.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = [torch.tensor(item[0]) for item in batch]\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1408414250.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [torch.tensor(item[1]) for item in batch]\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\3600398038.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(): # Mixed precision context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0, Loss: 1.1902\n",
      "Epoch 0 Batch 500, Loss: 0.9909\n",
      "Epoch 0 Batch 1000, Loss: 0.9449\n",
      "Epoch 0 Batch 1500, Loss: 1.0870\n",
      "Epoch 0 Batch 2000, Loss: 1.0481\n",
      "Epoch 0 Batch 2500, Loss: 0.8807\n",
      "Epoch 0 Batch 3000, Loss: 0.9844\n",
      "Epoch 0 Batch 3500, Loss: 1.2514\n",
      "Epoch 0 Batch 4000, Loss: 0.9097\n",
      "Epoch 0 Batch 4500, Loss: 0.9830\n",
      "Epoch 0 Batch 5000, Loss: 1.1111\n",
      "Mean Epoch Loss: 1.0328376115964701\n",
      "Epoch 1 Batch 0, Loss: 0.6671\n",
      "Epoch 1 Batch 500, Loss: 0.9785\n",
      "Epoch 1 Batch 1000, Loss: 0.8624\n",
      "Epoch 1 Batch 1500, Loss: 0.9475\n",
      "Epoch 1 Batch 2000, Loss: 1.0542\n",
      "Epoch 1 Batch 2500, Loss: 0.8508\n",
      "Epoch 1 Batch 3000, Loss: 0.7981\n",
      "Epoch 1 Batch 3500, Loss: 1.0148\n",
      "Epoch 1 Batch 4000, Loss: 0.9700\n",
      "Epoch 1 Batch 4500, Loss: 1.2245\n",
      "Epoch 1 Batch 5000, Loss: 0.9413\n",
      "Mean Epoch Loss: 0.9767361432069593\n",
      "Epoch 2 Batch 0, Loss: 0.8673\n",
      "Epoch 2 Batch 500, Loss: 0.9154\n",
      "Epoch 2 Batch 1000, Loss: 0.9781\n",
      "Epoch 2 Batch 1500, Loss: 1.1939\n",
      "Epoch 2 Batch 2000, Loss: 0.9804\n",
      "Epoch 2 Batch 2500, Loss: 0.7683\n",
      "Epoch 2 Batch 3000, Loss: 1.0045\n",
      "Epoch 2 Batch 3500, Loss: 0.7625\n",
      "Epoch 2 Batch 4000, Loss: 0.8226\n",
      "Epoch 2 Batch 4500, Loss: 0.8331\n",
      "Epoch 2 Batch 5000, Loss: 1.0087\n",
      "Mean Epoch Loss: 0.928016579874358\n",
      "Epoch 3 Batch 0, Loss: 0.9779\n",
      "Epoch 3 Batch 500, Loss: 0.7788\n",
      "Epoch 3 Batch 1000, Loss: 0.7771\n",
      "Epoch 3 Batch 1500, Loss: 0.9128\n",
      "Epoch 3 Batch 2000, Loss: 0.7697\n",
      "Epoch 3 Batch 2500, Loss: 1.6895\n",
      "Epoch 3 Batch 3000, Loss: 0.9911\n",
      "Epoch 3 Batch 3500, Loss: 0.8738\n",
      "Epoch 3 Batch 4000, Loss: 0.6826\n",
      "Epoch 3 Batch 4500, Loss: 0.6062\n",
      "Epoch 3 Batch 5000, Loss: 0.9356\n",
      "Mean Epoch Loss: 0.8846270847915626\n",
      "Epoch 4 Batch 0, Loss: 1.0665\n",
      "Epoch 4 Batch 500, Loss: 0.7565\n",
      "Epoch 4 Batch 1000, Loss: 0.7927\n",
      "Epoch 4 Batch 1500, Loss: 0.8648\n",
      "Epoch 4 Batch 2000, Loss: 0.9175\n",
      "Epoch 4 Batch 2500, Loss: 1.1274\n",
      "Epoch 4 Batch 3000, Loss: 0.8181\n",
      "Epoch 4 Batch 3500, Loss: 0.8879\n",
      "Epoch 4 Batch 4000, Loss: 0.9816\n",
      "Epoch 4 Batch 4500, Loss: 0.7663\n",
      "Epoch 4 Batch 5000, Loss: 0.7975\n",
      "Mean Epoch Loss: 0.8478146616086475\n",
      "Epoch 5 Batch 0, Loss: 0.8004\n",
      "Epoch 5 Batch 500, Loss: 0.8776\n",
      "Epoch 5 Batch 1000, Loss: 0.9037\n",
      "Epoch 5 Batch 1500, Loss: 0.9648\n",
      "Epoch 5 Batch 2000, Loss: 0.9369\n",
      "Epoch 5 Batch 2500, Loss: 0.9028\n",
      "Epoch 5 Batch 3000, Loss: 0.7494\n",
      "Epoch 5 Batch 3500, Loss: 0.7903\n",
      "Epoch 5 Batch 4000, Loss: 0.9777\n",
      "Epoch 5 Batch 4500, Loss: 0.6132\n",
      "Epoch 5 Batch 5000, Loss: 0.9828\n",
      "Mean Epoch Loss: 0.8138321022731964\n",
      "Epoch 6 Batch 0, Loss: 0.7238\n",
      "Epoch 6 Batch 500, Loss: 0.7716\n",
      "Epoch 6 Batch 1000, Loss: 0.7203\n",
      "Epoch 6 Batch 1500, Loss: 0.8799\n",
      "Epoch 6 Batch 2000, Loss: 0.7610\n",
      "Epoch 6 Batch 2500, Loss: 0.6547\n",
      "Epoch 6 Batch 3000, Loss: 0.7865\n",
      "Epoch 6 Batch 3500, Loss: 0.6673\n",
      "Epoch 6 Batch 4000, Loss: 0.8774\n",
      "Epoch 6 Batch 4500, Loss: 0.8781\n",
      "Epoch 6 Batch 5000, Loss: 0.7817\n",
      "Mean Epoch Loss: 0.7831172993772124\n",
      "Epoch 7 Batch 0, Loss: 0.9400\n",
      "Epoch 7 Batch 500, Loss: 0.7382\n",
      "Epoch 7 Batch 1000, Loss: 0.6852\n",
      "Epoch 7 Batch 1500, Loss: 0.8714\n",
      "Epoch 7 Batch 2000, Loss: 0.8595\n",
      "Epoch 7 Batch 2500, Loss: 0.8178\n",
      "Epoch 7 Batch 3000, Loss: 0.7846\n",
      "Epoch 7 Batch 3500, Loss: 0.9535\n",
      "Epoch 7 Batch 4000, Loss: 0.6167\n",
      "Epoch 7 Batch 4500, Loss: 0.8682\n",
      "Epoch 7 Batch 5000, Loss: 0.6745\n",
      "Mean Epoch Loss: 0.7552695607504877\n",
      "Epoch 8 Batch 0, Loss: 0.5728\n",
      "Epoch 8 Batch 500, Loss: 0.7955\n",
      "Epoch 8 Batch 1000, Loss: 0.6976\n",
      "Epoch 8 Batch 1500, Loss: 0.5264\n",
      "Epoch 8 Batch 2000, Loss: 0.7721\n",
      "Epoch 8 Batch 2500, Loss: 0.8044\n",
      "Epoch 8 Batch 3000, Loss: 0.8425\n",
      "Epoch 8 Batch 3500, Loss: 0.8331\n",
      "Epoch 8 Batch 4000, Loss: 0.6627\n",
      "Epoch 8 Batch 4500, Loss: 0.9063\n",
      "Epoch 8 Batch 5000, Loss: 0.7373\n",
      "Mean Epoch Loss: 0.729977823466763\n",
      "Epoch 9 Batch 0, Loss: 0.9481\n",
      "Epoch 9 Batch 500, Loss: 0.7290\n",
      "Epoch 9 Batch 1000, Loss: 0.4914\n",
      "Epoch 9 Batch 1500, Loss: 0.6255\n",
      "Epoch 9 Batch 2000, Loss: 0.7474\n",
      "Epoch 9 Batch 2500, Loss: 0.9077\n",
      "Epoch 9 Batch 3000, Loss: 0.8390\n",
      "Epoch 9 Batch 3500, Loss: 0.6438\n",
      "Epoch 9 Batch 4000, Loss: 0.6236\n",
      "Epoch 9 Batch 4500, Loss: 0.8878\n",
      "Epoch 9 Batch 5000, Loss: 0.5907\n",
      "Mean Epoch Loss: 0.7070716145336574\n",
      "Epoch 10 Batch 0, Loss: 0.8082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     train_one_epoch(model,haha, optimizer, criterion, epoch)\n",
      "Cell \u001b[1;32mIn[35], line 23\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, iterator, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m trg[:, \u001b[38;5;241m1\u001b[39m:] \n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(): \u001b[38;5;66;03m# Mixed precision context\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(src, tgt_input)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Output shape: [batch, seq_len-1, vocab_size]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Reshape for Loss: [batch * (seq_len-1), vocab_size]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 48\u001b[0m, in \u001b[0;36mTransformerSeq2Seq.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     45\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_emb(tgt))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Pass through Transformer\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m     49\u001b[0m     src\u001b[38;5;241m=\u001b[39msrc_emb,\n\u001b[0;32m     50\u001b[0m     tgt\u001b[38;5;241m=\u001b[39mtgt_emb,\n\u001b[0;32m     51\u001b[0m     tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, \u001b[38;5;66;03m# Mask future positions\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_padding_mask,\n\u001b[0;32m     53\u001b[0m     tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_padding_mask,\n\u001b[0;32m     54\u001b[0m     memory_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_padding_mask \u001b[38;5;66;03m# Mask source padding in attention\u001b[39;00m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Final Projection\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(outs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:278\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    272\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    273\u001b[0m     src,\n\u001b[0;32m    274\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[0;32m    275\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    276\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal,\n\u001b[0;32m    277\u001b[0m )\n\u001b[1;32m--> 278\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m    279\u001b[0m     tgt,\n\u001b[0;32m    280\u001b[0m     memory,\n\u001b[0;32m    281\u001b[0m     tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[0;32m    282\u001b[0m     memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    283\u001b[0m     tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    284\u001b[0m     memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    285\u001b[0m     tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[0;32m    286\u001b[0m     memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    599\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    603\u001b[0m         output,\n\u001b[0;32m    604\u001b[0m         memory,\n\u001b[0;32m    605\u001b[0m         tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[0;32m    606\u001b[0m         memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[0;32m    607\u001b[0m         tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    608\u001b[0m         memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[0;32m    609\u001b[0m         tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[0;32m    610\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[0;32m    611\u001b[0m     )\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1087\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m-> 1087\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[0;32m   1088\u001b[0m     )\n\u001b[0;32m   1089\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[0;32m   1090\u001b[0m         x\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[0;32m   1092\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m     )\n\u001b[0;32m   1095\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1107\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1102\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m   1108\u001b[0m         x,\n\u001b[0;32m   1109\u001b[0m         x,\n\u001b[0;32m   1110\u001b[0m         x,\n\u001b[0;32m   1111\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1112\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1113\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1114\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1115\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1369\u001b[0m         query,\n\u001b[0;32m   1370\u001b[0m         key,\n\u001b[0;32m   1371\u001b[0m         value,\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[0;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1382\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1383\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1384\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1385\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1386\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1387\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1388\u001b[0m     )\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:6278\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6275\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   6276\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 6278\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[0;32m   6279\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[0;32m   6280\u001b[0m )\n\u001b[0;32m   6281\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   6282\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   6283\u001b[0m )\n\u001b[0;32m   6285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_one_epoch(model,haha, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd85a9-932c-4553-aca3-fef034713c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebb489-b638-4485-80f6-01d2bb3659f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10a4c7-acd0-489b-84da-fe92ee489166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695721c-c3a9-48cc-bbcc-ab1dd54274ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2d3f2-e43d-48c2-84f2-31ee5977c5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ed023df3-7bd4-446f-ab50-012c659bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': 20,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, \"TransformerFR_ENGMODELANDALL.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "785d8bba-97c2-4072-8fbf-cc464a4e71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'just_model':model.state_dict()},\"modelkek.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b7e3f8f-d6da-4da4-af41-912f1af43436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: je vais beaucoup , de toi ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"I am doing great , WHat about you?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbb0ee85-442a-4a6c-b716-60b44e0942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: aujourd'hui est une bonne journée .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Today's a good day.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9141b069-f181-4020-af2b-09da36289d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: comment s' est échappé   ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"How was prison?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "55df86d9-181f-4520-b800-706bab33aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: mords la balle .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Bite the bullet.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47dd041d-22f5-4e66-a5fe-4cb4c34dff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: à quelle heure est -il ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"what's the time?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb85ec46-0737-432c-b469-a84cd8cb2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: quelle heure est -il .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"what's the time\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "298b7553-6b90-41b4-933f-4c9cb4316c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: à quelle heure est -il ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"whats the time?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cb60e3f-c194-4343-ac3d-f177bd171cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: comporte - toi !\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"behave yourself!\")\n",
    "print(f\"Translation: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68f7583f-2c2b-4990-b4e3-b40b781745e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: arrête , s' il te plaît !\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"please stop!\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e88ecf3-e377-46b2-abd9-7d7006547e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: arrête , s' il te plaît , arrêter .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"please stop\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc11dc32-99d7-43f7-a610-052579da6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: soyez très prudentes des abeilles .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Be very careful of bees.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74fc2660-c787-4740-abe0-de8792c50115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est complètement absurde .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's all nonsense.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd4d20de-6d1a-47ec-a7aa-32b5a29820ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>Why is this funny?</td>\n",
       "      <td>Pourquoi c'est marrant ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22001</th>\n",
       "      <td>Why must I suffer?</td>\n",
       "      <td>Pourquoi dois-je souffrir ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22002</th>\n",
       "      <td>Why must we do it?</td>\n",
       "      <td>Pourquoi devons-nous le faire ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22003</th>\n",
       "      <td>Why not just quit?</td>\n",
       "      <td>Pourquoi ne pas simplement démissionner ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22004</th>\n",
       "      <td>Why not just quit?</td>\n",
       "      <td>Pourquoi ne pas simplement arrêter ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English words/sentences                     French words/sentences\n",
       "22000      Why is this funny?                   Pourquoi c'est marrant ?\n",
       "22001      Why must I suffer?                Pourquoi dois-je souffrir ?\n",
       "22002      Why must we do it?            Pourquoi devons-nous le faire ?\n",
       "22003      Why not just quit?  Pourquoi ne pas simplement démissionner ?\n",
       "22004      Why not just quit?       Pourquoi ne pas simplement arrêter ?"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[22000:22005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88ecb622-6807-487d-8897-f9095578e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est un pays marrant .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's a funny country.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b73a3d5-556e-4838-8acf-5d2d3eacd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>They are very kind.</td>\n",
       "      <td>Ils sont très gentils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26001</th>\n",
       "      <td>They are very kind.</td>\n",
       "      <td>Elles sont très gentilles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26002</th>\n",
       "      <td>They can't do that.</td>\n",
       "      <td>Ils n'arrivent pas à faire ça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26003</th>\n",
       "      <td>They can't do that.</td>\n",
       "      <td>Ils n'arrivent pas à le faire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26004</th>\n",
       "      <td>They can't do that.</td>\n",
       "      <td>Ils n'y arrivent pas.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English words/sentences          French words/sentences\n",
       "26000     They are very kind.          Ils sont très gentils.\n",
       "26001     They are very kind.      Elles sont très gentilles.\n",
       "26002     They can't do that.  Ils n'arrivent pas à faire ça.\n",
       "26003     They can't do that.  Ils n'arrivent pas à le faire.\n",
       "26004     They can't do that.           Ils n'y arrivent pas."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[26000:26005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7bd6cea4-53a0-41e1-9c24-10b7cbd36ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: ne fais pas ça .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Don't do that.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34ffbbf5-4a0c-4bbe-9314-97a081ab0b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est très gentil de votre part .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"That's very kind of you.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74070cee-8557-4a5d-a690-bff276dfae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7dbb72a-0e35-4c40-ace1-02fe15129133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est une distraction .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's a distraction.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5de9fe89-6dc9-43ab-b05d-e9db8c9c89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: pourquoi ne pas simplement démissionner ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Why not just quit?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32df866a-8503-4543-82e5-af580a2d42bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: pourquoi dois -je souffrir ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Why must I suffer?\")\n",
    "print(f\"Translation: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19f543-13ec-4f03-838f-b7bb67a40d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc887d-532b-4dc8-b93d-4573a28a8f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0576-54dc-4a1b-8360-4789e48908c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724a5e-792a-4dfe-a823-59b5adfa9f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
