{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03fd3d8b-bef9-4182-9c47-04ed5b94f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import Datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d41ae3-f85b-4266-976e-115410421201",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55619e2a-5124-4c0d-b365-a0268bffca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812e5bfb-6be0-4cf7-bd9a-838083f45a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('engfrench.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9de6cd-6070-4bb0-8a97-c5cfb6d3cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175621\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16df2aed-79f6-4a10-af85-e10d4a10a9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    The guards found a hacksaw blade in the prison...\n",
       "French words/sentences     Les gardiens trouvèrent une lame de scie à mét...\n",
       "Name: 170000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[170000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025efad2-8e80-47f1-a486-64a94073dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Be very careful.</td>\n",
       "      <td>Sois très prudente !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Bees make honey.</td>\n",
       "      <td>Les abeilles font du miel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Behave yourself.</td>\n",
       "      <td>Comporte-toi bien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Bite the bullet.</td>\n",
       "      <td>Serre les dents.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Bite the bullet.</td>\n",
       "      <td>Serrez les dents.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English words/sentences      French words/sentences\n",
       "10000        Be very careful.        Sois très prudente !\n",
       "10001        Bees make honey.  Les abeilles font du miel.\n",
       "10002        Behave yourself.          Comporte-toi bien.\n",
       "10003        Bite the bullet.            Serre les dents.\n",
       "10004        Bite the bullet.           Serrez les dents."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10000:10005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe24555-676c-440b-b2bd-4876a36cc782",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe1ff5-c1f9-4b8b-aec0-a4e907ffceb4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d31611e-192e-4b2a-bafb-06ca5a82ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID=0\n",
    "SOS_TOKEN_ID = 1\n",
    "EOS_TOKEN_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad1c64-dd2d-4601-b8f5-df4f66600163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ed24b-8349-4eb7-b1d3-bbc3e5a9aa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf3179-1b66-40fb-adac-a22ca0fc5971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f06527-e676-4c98-962a-2c4f64ec51b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8165ff4-0c6a-443e-b3bc-7b9defd201fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txten=data.iloc[0,0]\n",
    "txtfr=data.iloc[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841c7d65-e445-45da-9b8c-605193c87e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b3ef1e-ee68-42a6-add8-02108412f025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salut!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b9abb0-f1ad-400b-934e-4c6c46c165ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1505539583.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  en_check=torch.load('engEmbeddings.pt')\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1505539583.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fr_check=torch.load('frEmbeddings.pt')\n"
     ]
    }
   ],
   "source": [
    "en_check=torch.load('engEmbeddings.pt')\n",
    "fr_check=torch.load('frEmbeddings.pt')\n",
    "en_weights=en_check['weights']\n",
    "en_vocab=en_check['vocab_dict']\n",
    "fr_weights=fr_check['weights']\n",
    "fr_vocab=fr_check['vocab_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfb4c38-f5c7-4858-8734-f1ee732f9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, en_vocab, fr_vocab):\n",
    "        self.df = dataframe\n",
    "        self.en_vocab = en_vocab\n",
    "        self.fr_vocab = fr_vocab\n",
    "        \n",
    "        # Define special token IDs\n",
    "        # IMPORTANT: Ensure these keys exist in your vocab dictionaries!\n",
    "        self.unk_idx_en = en_vocab.get('<unk>', 0) # Default to 0 if not found\n",
    "        self.eos_idx_en = en_vocab.get('<eos>', 2)\n",
    "        \n",
    "        self.unk_idx_fr = fr_vocab.get('<unk>', 0)\n",
    "        self.sos_idx_fr = fr_vocab.get('<sos>', 1)\n",
    "        self.eos_idx_fr = fr_vocab.get('<eos>', 2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Get Raw Text\n",
    "        src_text = str(self.df.iloc[index, 0])\n",
    "        trg_text = str(self.df.iloc[index, 1])\n",
    "\n",
    "        # 2. Tokenize (using your Spacy functions)\n",
    "        # We use .lower() because FastText matches better with lowercase\n",
    "        src_tokens = tokenize_en(src_text.lower())\n",
    "        trg_tokens = tokenize_fr(trg_text.lower())\n",
    "\n",
    "        # 3. Numericalize Source (English)\n",
    "        # Use .get() to map unknown words to <unk> index\n",
    "        src_indices = [self.en_vocab.get(token, self.unk_idx_en) for token in src_tokens]\n",
    "        \n",
    "        # 4. Numericalize Target (French)\n",
    "        trg_indices = [self.fr_vocab.get(token, self.unk_idx_fr) for token in trg_tokens]\n",
    "\n",
    "        # 5. Add Special Tokens\n",
    "        # Source usually needs <eos> at the end so the LSTM knows the sentence stopped\n",
    "        src_indices.append(self.eos_idx_en)\n",
    "        \n",
    "        # Target needs <sos> to start generation and <eos> to end it\n",
    "        trg_indices = [self.sos_idx_fr] + trg_indices + [self.eos_idx_fr]\n",
    "\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(trg_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d80f4-c634-4be8-90ec-24c4e631caa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ea23429-fdac-48fe-829b-59ed9a153ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanewdataset=TranslationDataset(data,en_vocab,fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bccd8c-4881-409a-8e1f-e4e50fff387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # 'batch' is a list of tuples: [(input_tensor_1, target_tensor_1), (input_tensor_2, target_tensor_2), ...]\n",
    "\n",
    "    # 1. Separate inputs and targets\n",
    "    inputs = [torch.tensor(item[0]) for item in batch]\n",
    "    targets = [torch.tensor(item[1]) for item in batch]\n",
    "\n",
    "    # 2. Pad the inputs and targets to the length of the longest sequence in the batch\n",
    "    # batch_first=True makes the output shape (BatchSize, SequenceLength)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    \n",
    "    return inputs_padded, targets_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b51b6de-3ca3-481d-9119-6dc6772d3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "haha=DataLoader(datanewdataset,batch_size=32,shuffle=True,collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce07bd-19b3-43ad-a820-3e009ea3ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d26a01-a545-495f-b951-878ee45e667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b364586-8172-4ecf-96db-b1e781b20ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62a31a-6579-4f09-a820-810e546e45ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe745ea-9a11-4802-9f5f-cbf53428ed04",
   "metadata": {},
   "source": [
    "SAMPLE TO REMEMBER LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf9879c-de35-4cb7-b158-b51608fd6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa=torch.randn(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a435a35-e05b-4e98-8c7a-d28669943f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ha=nn.LSTM(100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e076d8cb-7094-4dea-8a85-af2cf4064cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ya=ha(xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f497155-a726-4de9-a4f9-dc4b85034d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3400ae-aaa7-4f69-965a-a7b0e64e4bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da92149f-7fcc-44a4-b1cb-ca91e63dee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063b89a-269e-4954-955d-11b2737534ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eee28f-7402-4731-b185-2ab135621136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a256-a5ee-432a-9aa0-9f8b3afcf0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01357329-82cc-4c9a-9a8b-b16da3c77f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=300\n",
    "#since using pre trained Fast TExt aligned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cc29a79-b82a-461d-bd5a-56f3f0f5809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_ENC=len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acde860c-f52a-4ac4-8001-d654390c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE_FR=len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95e5e1-d7c6-4a3e-a2dc-2e785ae9637a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd32de2a-d261-47bb-87cd-69cf7ffffa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a long constant matrix representing position\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a learnable parameter, but part of state_dict)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, d_model]\n",
    "        # Add position encoding to embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9442f3c2-c760-4990-88da-61b1c5415c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, device, embed_dim=300, nhead=4, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Embeddings (Using your pretrained weights)\n",
    "        self.src_emb = nn.Embedding.from_pretrained(en_weights, freeze=True)\n",
    "        self.tgt_emb = nn.Embedding.from_pretrained(fr_weights, freeze=True)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, dropout=dropout)\n",
    "        \n",
    "        # 3. The Transformer\n",
    "        # batch_first=True is crucial to match your [batch, len] data format\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        # 4. Output Layer\n",
    "        self.fc_out = nn.Linear(embed_dim, len(fr_vocab)) # Project to French Vocab size\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: [batch, src_len]\n",
    "        # tgt: [batch, tgt_len] (This is the \"Teacher Forcing\" input)\n",
    "        \n",
    "        # Create Masks (Crucial for Transformers!)\n",
    "        # src_mask = mask for padding tokens in source\n",
    "        # tgt_mask = \"Causal Mask\" (prevents decoder from seeing future words) + padding mask\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(self.device)\n",
    "        \n",
    "        # Generate Padding Masks (assuming 0 is your pad_token_id)\n",
    "        # These boolean masks tell the model to ignore padding tokens\n",
    "        src_padding_mask = (src == 0).to(self.device) \n",
    "        tgt_padding_mask = (tgt == 0).to(self.device)\n",
    "\n",
    "        # Apply Embeddings + Positional Encoding\n",
    "        src_emb = self.positional_encoding(self.src_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_emb(tgt))\n",
    "        \n",
    "        # Pass through Transformer\n",
    "        outs = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=tgt_mask, # Mask future positions\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask # Mask source padding in attention\n",
    "        )\n",
    "        \n",
    "        # Final Projection\n",
    "        return self.fc_out(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b01eec7-5d21-4ade-97c3-5af4f22a315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # RTX 4070 supports Mixed Precision (faster training)\n",
    "    scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device) # trg shape: [batch, trg_len] e.g., <sos> ... <eos>\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare Inputs and Targets for Transformer\n",
    "        # Decoder Input: Remove the LAST token (<eos>)\n",
    "        tgt_input = trg[:, :-1] \n",
    "        \n",
    "        # Expected Output: Remove the FIRST token (<sos>)\n",
    "        tgt_output = trg[:, 1:] \n",
    "        \n",
    "        with torch.cuda.amp.autocast(): # Mixed precision context\n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            # Output shape: [batch, seq_len-1, vocab_size]\n",
    "            # Reshape for Loss: [batch * (seq_len-1), vocab_size]\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backprop with Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip Gradients (Good stability for Transformers too)\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Mean Epoch Loss: {epoch_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85ae9e74-19c8-48d8-9779-4b37465eee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_translation_transformer(model, sentence, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Preprocess (Tokenize + Numericalize)\n",
    "    tokens = tokenize_en(sentence.lower())\n",
    "    unk_idx = en_vocab.get('<unk>', 3)\n",
    "    eos_idx = en_vocab.get('<eos>', 2)\n",
    "    ids = [en_vocab.get(token, unk_idx) for token in tokens]\n",
    "    ids.append(eos_idx) # Add <eos> to source\n",
    "    \n",
    "    src_tensor = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 2. The Loop\n",
    "    # Start with just the <sos> token\n",
    "    sos_idx = fr_vocab['<sos>']\n",
    "    eos_idx = fr_vocab['<eos>']\n",
    "    \n",
    "    tgt_indices = [sos_idx]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get masks inside the model forward (or create dummy masks here)\n",
    "            # We call the model. The model will create the causal mask internally\n",
    "            output = model(src_tensor, tgt_tensor)\n",
    "        \n",
    "        # We only care about the prediction for the LAST token\n",
    "        # output shape: [1, len, vocab]\n",
    "        last_token_logits = output[:, -1, :]\n",
    "        pred_token = last_token_logits.argmax(1).item()\n",
    "        \n",
    "        tgt_indices.append(pred_token)\n",
    "        \n",
    "        if pred_token == eos_idx:\n",
    "            break\n",
    "            \n",
    "    # 3. Decode to Words\n",
    "    idx_to_word = {v: k for k, v in fr_vocab.items()}\n",
    "    predicted_tokens = [idx_to_word.get(idx, '<unk>') for idx in tgt_indices[1:]] # Skip <sos>\n",
    "    \n",
    "    # Remove <eos> from display if present\n",
    "    if predicted_tokens and predicted_tokens[-1] == '<eos>':\n",
    "        predicted_tokens = predicted_tokens[:-1]\n",
    "        \n",
    "    return \" \".join(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be6cc9f8-b1cf-48d9-b33d-c774c8b3ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "856766e1-5deb-4792-b5d9-0b320cbc6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on: {torch.cuda.get_device_name(0)}\") # Should say RTX 4070\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "    device=device,\n",
    "    embed_dim=embed_dim, # e.g. 300\n",
    "    nhead=4,             # 300 / 4 = 75\n",
    "    num_layers=3,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Assuming 0 is pad_index\n",
    "\n",
    "# Run Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7910d65-c214-4632-80f1-789c125c46a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\3600398038.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1408414250.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = [torch.tensor(item[0]) for item in batch]\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\1408414250.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [torch.tensor(item[1]) for item in batch]\n",
      "C:\\Users\\maddo\\AppData\\Local\\Temp\\ipykernel_19472\\3600398038.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(): # Mixed precision context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0, Loss: 10.2131\n",
      "Epoch 0 Batch 500, Loss: 5.1015\n",
      "Epoch 0 Batch 1000, Loss: 4.5509\n",
      "Epoch 0 Batch 1500, Loss: 4.4730\n",
      "Epoch 0 Batch 2000, Loss: 4.4362\n",
      "Epoch 0 Batch 2500, Loss: 4.2060\n",
      "Epoch 0 Batch 3000, Loss: 3.8827\n",
      "Epoch 0 Batch 3500, Loss: 4.1893\n",
      "Epoch 0 Batch 4000, Loss: 4.0161\n",
      "Epoch 0 Batch 4500, Loss: 3.6453\n",
      "Epoch 0 Batch 5000, Loss: 3.6865\n",
      "Mean Epoch Loss: 4.175788068753988\n",
      "Epoch 1 Batch 0, Loss: 3.2777\n",
      "Epoch 1 Batch 500, Loss: 3.3258\n",
      "Epoch 1 Batch 1000, Loss: 3.3926\n",
      "Epoch 1 Batch 1500, Loss: 3.2175\n",
      "Epoch 1 Batch 2000, Loss: 3.0209\n",
      "Epoch 1 Batch 2500, Loss: 3.1661\n",
      "Epoch 1 Batch 3000, Loss: 3.4206\n",
      "Epoch 1 Batch 3500, Loss: 2.8804\n",
      "Epoch 1 Batch 4000, Loss: 2.5529\n",
      "Epoch 1 Batch 4500, Loss: 2.7795\n",
      "Epoch 1 Batch 5000, Loss: 2.7276\n",
      "Mean Epoch Loss: 3.0520717572983465\n",
      "Epoch 2 Batch 0, Loss: 2.0434\n",
      "Epoch 2 Batch 500, Loss: 2.9962\n",
      "Epoch 2 Batch 1000, Loss: 2.7126\n",
      "Epoch 2 Batch 1500, Loss: 2.6753\n",
      "Epoch 2 Batch 2000, Loss: 2.2042\n",
      "Epoch 2 Batch 2500, Loss: 2.4954\n",
      "Epoch 2 Batch 3000, Loss: 2.5261\n",
      "Epoch 2 Batch 3500, Loss: 2.5164\n",
      "Epoch 2 Batch 4000, Loss: 2.2490\n",
      "Epoch 2 Batch 4500, Loss: 2.3216\n",
      "Epoch 2 Batch 5000, Loss: 2.5069\n",
      "Mean Epoch Loss: 2.428164036574966\n",
      "Epoch 3 Batch 0, Loss: 2.4957\n",
      "Epoch 3 Batch 500, Loss: 1.9728\n",
      "Epoch 3 Batch 1000, Loss: 2.2312\n",
      "Epoch 3 Batch 1500, Loss: 2.1076\n",
      "Epoch 3 Batch 2000, Loss: 2.2877\n",
      "Epoch 3 Batch 2500, Loss: 1.7285\n",
      "Epoch 3 Batch 3000, Loss: 1.9549\n",
      "Epoch 3 Batch 3500, Loss: 2.1570\n",
      "Epoch 3 Batch 4000, Loss: 2.0311\n",
      "Epoch 3 Batch 4500, Loss: 1.9703\n",
      "Epoch 3 Batch 5000, Loss: 1.8594\n",
      "Mean Epoch Loss: 2.0068413035468686\n",
      "Epoch 4 Batch 0, Loss: 1.2672\n",
      "Epoch 4 Batch 500, Loss: 1.9603\n",
      "Epoch 4 Batch 1000, Loss: 1.7798\n",
      "Epoch 4 Batch 1500, Loss: 1.9444\n",
      "Epoch 4 Batch 2000, Loss: 1.3936\n",
      "Epoch 4 Batch 2500, Loss: 2.0342\n",
      "Epoch 4 Batch 3000, Loss: 1.5149\n",
      "Epoch 4 Batch 3500, Loss: 1.9365\n",
      "Epoch 4 Batch 4000, Loss: 1.9358\n",
      "Epoch 4 Batch 4500, Loss: 1.9902\n",
      "Epoch 4 Batch 5000, Loss: 1.5091\n",
      "Mean Epoch Loss: 1.7232550263383128\n",
      "Epoch 5 Batch 0, Loss: 1.8836\n",
      "Epoch 5 Batch 500, Loss: 1.4530\n",
      "Epoch 5 Batch 1000, Loss: 1.6206\n",
      "Epoch 5 Batch 1500, Loss: 1.7668\n",
      "Epoch 5 Batch 2000, Loss: 1.2620\n",
      "Epoch 5 Batch 2500, Loss: 1.4729\n",
      "Epoch 5 Batch 3000, Loss: 1.6042\n",
      "Epoch 5 Batch 3500, Loss: 1.8142\n",
      "Epoch 5 Batch 4000, Loss: 1.4068\n",
      "Epoch 5 Batch 4500, Loss: 1.3593\n",
      "Epoch 5 Batch 5000, Loss: 1.3756\n",
      "Mean Epoch Loss: 1.5250440732467194\n",
      "Epoch 6 Batch 0, Loss: 1.5935\n",
      "Epoch 6 Batch 500, Loss: 1.4837\n",
      "Epoch 6 Batch 1000, Loss: 1.3216\n",
      "Epoch 6 Batch 1500, Loss: 1.3945\n",
      "Epoch 6 Batch 2000, Loss: 1.4416\n",
      "Epoch 6 Batch 2500, Loss: 1.1061\n",
      "Epoch 6 Batch 3000, Loss: 1.2742\n",
      "Epoch 6 Batch 3500, Loss: 1.2566\n",
      "Epoch 6 Batch 4000, Loss: 1.3502\n",
      "Epoch 6 Batch 4500, Loss: 1.2783\n",
      "Epoch 6 Batch 5000, Loss: 1.4171\n",
      "Mean Epoch Loss: 1.378364544951605\n",
      "Epoch 7 Batch 0, Loss: 1.4736\n",
      "Epoch 7 Batch 500, Loss: 1.1535\n",
      "Epoch 7 Batch 1000, Loss: 1.5742\n",
      "Epoch 7 Batch 1500, Loss: 1.0330\n",
      "Epoch 7 Batch 2000, Loss: 1.3815\n",
      "Epoch 7 Batch 2500, Loss: 1.2679\n",
      "Epoch 7 Batch 3000, Loss: 1.2495\n",
      "Epoch 7 Batch 3500, Loss: 1.1764\n",
      "Epoch 7 Batch 4000, Loss: 0.9661\n",
      "Epoch 7 Batch 4500, Loss: 1.2860\n",
      "Epoch 7 Batch 5000, Loss: 1.3551\n",
      "Mean Epoch Loss: 1.264881564850233\n",
      "Epoch 8 Batch 0, Loss: 0.9159\n",
      "Epoch 8 Batch 500, Loss: 1.1198\n",
      "Epoch 8 Batch 1000, Loss: 1.2725\n",
      "Epoch 8 Batch 1500, Loss: 1.2288\n",
      "Epoch 8 Batch 2000, Loss: 1.5928\n",
      "Epoch 8 Batch 2500, Loss: 1.6087\n",
      "Epoch 8 Batch 3000, Loss: 1.1094\n",
      "Epoch 8 Batch 3500, Loss: 1.1212\n",
      "Epoch 8 Batch 4000, Loss: 1.1476\n",
      "Epoch 8 Batch 4500, Loss: 1.0947\n",
      "Epoch 8 Batch 5000, Loss: 1.2401\n",
      "Mean Epoch Loss: 1.1733181655005172\n",
      "Epoch 9 Batch 0, Loss: 1.2726\n",
      "Epoch 9 Batch 500, Loss: 1.2075\n",
      "Epoch 9 Batch 1000, Loss: 0.8107\n",
      "Epoch 9 Batch 1500, Loss: 1.0120\n",
      "Epoch 9 Batch 2000, Loss: 1.0523\n",
      "Epoch 9 Batch 2500, Loss: 1.3256\n",
      "Epoch 9 Batch 3000, Loss: 1.3002\n",
      "Epoch 9 Batch 3500, Loss: 1.0149\n",
      "Epoch 9 Batch 4000, Loss: 1.1077\n",
      "Epoch 9 Batch 4500, Loss: 1.0356\n",
      "Epoch 9 Batch 5000, Loss: 0.9375\n",
      "Mean Epoch Loss: 1.0969123192054149\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_one_epoch(model,haha, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd85a9-932c-4553-aca3-fef034713c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbebb489-b638-4485-80f6-01d2bb3659f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10a4c7-acd0-489b-84da-fe92ee489166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695721c-c3a9-48cc-bbcc-ab1dd54274ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2d3f2-e43d-48c2-84f2-31ee5977c5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed023df3-7bd4-446f-ab50-012c659bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': 11,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, \"TransformerFR_ENGMODELANDALL.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d8bba-97c2-4072-8fbf-cc464a4e71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'just_model':model.state_dict()},\"modelkek.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7322657-364c-4f83-aaf4-2661b67bae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_translation_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b7e3f8f-d6da-4da4-af41-912f1af43436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: je fais bien , de quoi vous ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"I am doing great , WHat about you?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbb0ee85-442a-4a6c-b716-60b44e0942c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: aujourd'hui est un bon jour .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Today's a good day.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9141b069-f181-4020-af2b-09da36289d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: comment s' est -elle échappé ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"How was prison?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55df86d9-181f-4520-b800-706bab33aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: mords la flèche .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Bite the bullet.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47dd041d-22f5-4e66-a5fe-4cb4c34dff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: quelle est le temps ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"what's the time?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb85ec46-0737-432c-b469-a84cd8cb2e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: quelle est le temps .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"what's the time\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "298b7553-6b90-41b4-933f-4c9cb4316c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: à quoi remonte le temps ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"whats the time?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cb60e3f-c194-4343-ac3d-f177bd171cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: fais attention !\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"behave yourself!\")\n",
    "print(f\"Translation: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68f7583f-2c2b-4990-b4e3-b40b781745e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: arrête - toi , je te prie !\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"please stop!\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e88ecf3-e377-46b2-abd9-7d7006547e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: arrête , s' il te plaît .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"please stop\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc11dc32-99d7-43f7-a610-052579da6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: soyez très prudentes des abeilles .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Be very careful of bees.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74fc2660-c787-4740-abe0-de8792c50115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est tout faux .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's all nonsense.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd4d20de-6d1a-47ec-a7aa-32b5a29820ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>Why is this funny?</td>\n",
       "      <td>Pourquoi c'est marrant ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22001</th>\n",
       "      <td>Why must I suffer?</td>\n",
       "      <td>Pourquoi dois-je souffrir ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22002</th>\n",
       "      <td>Why must we do it?</td>\n",
       "      <td>Pourquoi devons-nous le faire ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22003</th>\n",
       "      <td>Why not just quit?</td>\n",
       "      <td>Pourquoi ne pas simplement démissionner ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22004</th>\n",
       "      <td>Why not just quit?</td>\n",
       "      <td>Pourquoi ne pas simplement arrêter ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English words/sentences                     French words/sentences\n",
       "22000      Why is this funny?                   Pourquoi c'est marrant ?\n",
       "22001      Why must I suffer?                Pourquoi dois-je souffrir ?\n",
       "22002      Why must we do it?            Pourquoi devons-nous le faire ?\n",
       "22003      Why not just quit?  Pourquoi ne pas simplement démissionner ?\n",
       "22004      Why not just quit?       Pourquoi ne pas simplement arrêter ?"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data[22000:22005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88ecb622-6807-487d-8897-f9095578e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est un pays marrant .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maddo\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's a funny country.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7dbb72a-0e35-4c40-ace1-02fe15129133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: c' est une distraction .\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"It's a distraction.\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5de9fe89-6dc9-43ab-b05d-e9db8c9c89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: pourquoi ne pas simplement arrêter ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Why not just quit?\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32df866a-8503-4543-82e5-af580a2d42bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: pourquoi dois -je souffrir ?\n"
     ]
    }
   ],
   "source": [
    "translation = predict_translation_transformer(model, \"Why must I suffer?\")\n",
    "print(f\"Translation: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19f543-13ec-4f03-838f-b7bb67a40d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc887d-532b-4dc8-b93d-4573a28a8f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0576-54dc-4a1b-8360-4789e48908c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724a5e-792a-4dfe-a823-59b5adfa9f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
