Massive difference with pre trained embeddings and prediction for English with proper punctuations . 


<img width="1315" height="712" alt="image" src="https://github.com/user-attachments/assets/a3c09e47-8a9d-42e3-830b-6d815f9f6e2e" />


After adding attention With LSTM , the loss is higher for same epoch but thats expected , GOnna train more to seee if the translates get better: 

<img width="1353" height="803" alt="image" src="https://github.com/user-attachments/assets/d2827ff8-6760-482e-9c87-46934e0e869c" />



With transformer at 20 ePoch it gives great results : 

<img width="1299" height="796" alt="Screenshot 2025-12-06 210804" src="https://github.com/user-attachments/assets/2afea5e3-33ff-438c-9d8a-e155eee52440" />
